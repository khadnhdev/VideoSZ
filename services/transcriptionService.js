const OpenAI = require('openai');
const fs = require('fs');
const Video = require('../models/Video');

class TranscriptionService {
  constructor() {
    // Cáº¥u hÃ¬nh OpenAI vá»›i kháº£ nÄƒng tÃ¹y chá»‰nh endpoint
    const openAIConfig = {
      apiKey: process.env.OPENAI_API_KEY,
    };
    
    // ThÃªm baseURL náº¿u Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh trong biáº¿n mÃ´i trÆ°á»ng
    if (process.env.OPENAI_API_ENDPOINT) {
      openAIConfig.baseURL = process.env.OPENAI_API_ENDPOINT;
      console.log(`ğŸ”— Sá»­ dá»¥ng OpenAI API endpoint tÃ¹y chá»‰nh: ${process.env.OPENAI_API_ENDPOINT}`);
    }
    
    this.openai = new OpenAI(openAIConfig);
  }

  async transcribeAudio(videoId) {
    const startTime = new Date();
    console.log(`\nğŸ™ï¸ Báº®T Äáº¦U TRANSCRIBE AUDIO (${startTime.toISOString()})`);
    
    try {
      const video = await Video.findById(videoId);
      if (!video || !video.audio_path) {
        console.error('âŒ VIDEO HOáº¶C AUDIO KHÃ”NG Tá»’N Táº I:', videoId);
        throw new Error('Video hoáº·c audio khÃ´ng tá»“n táº¡i');
      }
      console.log(`ğŸ“ ThÃ´ng tin audio: ID=${videoId}, Path=${video.audio_path}`);

      // Kiá»ƒm tra file audio tá»“n táº¡i
      if (!fs.existsSync(video.audio_path)) {
        console.error(`âŒ FILE AUDIO KHÃ”NG Tá»’N Táº I: ${video.audio_path}`);
        throw new Error(`File audio khÃ´ng tá»“n táº¡i: ${video.audio_path}`);
      }
      
      const stats = fs.statSync(video.audio_path);
      console.log(`ğŸ“Š KÃ­ch thÆ°á»›c file audio: ${(stats.size / (1024 * 1024)).toFixed(2)} MB`);

      // Äá»c file audio
      console.log('ğŸ“‚ Äang Ä‘á»c file audio...');
      const audioFile = fs.createReadStream(video.audio_path);

      // Gá»i API OpenAI Ä‘á»ƒ transcribe
      console.log('ğŸŒ Äang gá»i API OpenAI Whisper...');
      console.log(`âš™ï¸ Sá»­ dá»¥ng model: ${process.env.OPENAI_WHISPER_MODEL || "whisper-1"}, ngÃ´n ngá»¯: vi`);
      
      const response = await this.openai.audio.transcriptions.create({
        file: audioFile,
        model: process.env.OPENAI_WHISPER_MODEL || "whisper-1",
        language: "vi",
        response_format: "json",
      });

      // LÆ°u transcript vÃ o database
      const transcript = response.text;
      console.log(`âœ… Transcribe thÃ nh cÃ´ng: ${transcript.length} kÃ½ tá»±`);
      console.log(`ğŸ“ Äoáº¡n Ä‘áº§u transcript: "${transcript.substring(0, 100)}..."`);
      
      console.log('ğŸ’¾ Äang lÆ°u transcript vÃ o database...');
      await Video.updateTranscript(videoId, transcript);
      console.log('âœ… ÄÃ£ lÆ°u transcript vÃ o database');

      const endTime = new Date();
      const duration = (endTime - startTime) / 1000;
      console.log(`â±ï¸ Thá»i gian transcribe: ${duration.toFixed(2)} giÃ¢y`);
      console.log('ğŸ™ï¸ Káº¾T THÃšC TRANSCRIBE AUDIO');

      return transcript;
    } catch (error) {
      console.error('âŒ Lá»–I TRONG TRANSCRIBE_AUDIO:', error);
      throw error;
    }
  }
}

module.exports = new TranscriptionService(); 